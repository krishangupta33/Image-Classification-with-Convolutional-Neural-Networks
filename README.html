<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>CIFAR-10 Image Classification with Convolutional Neural Networks</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h1 id="cifar-10-image-classification-with-convolutional-neural-networks">CIFAR-10 Image Classification with Convolutional Neural Networks</h1>
<h2 id="objective">Objective</h2>
<p>The objective of this project is to experiment with different Convolutional Neural Network (CNN) architectures for image classification on the CIFAR-10 dataset. This involves varying aspects such as the use of pooling layers, fully connected layers, different kernel sizes, and number of channels. The goal is to evaluate the performance of these architectures based on metrics such as accuracy and precision.</p>
<h2 id="dataset">Dataset</h2>
<p>The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The classes are:</p>
<ol start="0">
<li>airplane</li>
<li>automobile</li>
<li>bird</li>
<li>cat</li>
<li>deer</li>
<li>dog</li>
<li>frog</li>
<li>horse</li>
<li>ship</li>
<li>truck</li>
</ol>
<p>Sample data from the CIFAR-10 dataset:</p>
<p><img src="file:////Users/krishan/Documents/GitHub/Image-Classification-with-Convolutional-Neural-Networks/cifar10sample.png" alt="sample images"></p>
<h2 id="steps">Steps</h2>
<ol>
<li>Load and preprocess the CIFAR-10 dataset</li>
<li>Implement different CNN architectures</li>
<li>Train the CNN models</li>
<li>Evaluate the trained models on the test set</li>
<li>Compare the performance of different architectures</li>
</ol>
<h2 id="loading-and-preprocessing-the-dataset">Loading and Preprocessing the Dataset</h2>
<p>The CIFAR-10 dataset is loaded using the <code>torchvision</code> library in PyTorch. The images are normalized and transformed to tensors. The dataset is split into training and test sets, and data loaders are created with a specified batch size.</p>
<pre><code class="language-python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CIFAR10DataModule</span>(pl.LightningDataModule):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, batch_size=<span class="hljs-number">32</span></span>):
        <span class="hljs-built_in">super</span>().__init__()
        self.batch_size = batch_size
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((<span class="hljs-number">0.4914</span>, <span class="hljs-number">0.4822</span>, <span class="hljs-number">0.4465</span>), (<span class="hljs-number">0.247</span>, <span class="hljs-number">0.243</span>, <span class="hljs-number">0.261</span>))
        ])

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_data</span>(<span class="hljs-params">self</span>):
        <span class="hljs-comment"># Download CIFAR-10 dataset</span>
        CIFAR10(root=<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>)
        CIFAR10(root=<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">False</span>, download=<span class="hljs-literal">True</span>)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">setup</span>(<span class="hljs-params">self, stage=<span class="hljs-literal">None</span></span>):
        <span class="hljs-comment"># Split dataset into train and validation sets</span>
        <span class="hljs-keyword">if</span> stage == <span class="hljs-string">&#x27;fit&#x27;</span> <span class="hljs-keyword">or</span> stage <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            cifar_full = CIFAR10(root=<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">True</span>, transform=self.transform)
            self.cifar_train, self.cifar_val = random_split(cifar_full, [<span class="hljs-number">45000</span>, <span class="hljs-number">5000</span>])

        <span class="hljs-keyword">if</span> stage == <span class="hljs-string">&#x27;test&#x27;</span> <span class="hljs-keyword">or</span> stage <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            self.cifar_test = CIFAR10(root=<span class="hljs-string">&quot;data&quot;</span>, train=<span class="hljs-literal">False</span>, transform=self.transform)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train_dataloader</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> DataLoader(self.cifar_train, batch_size=self.batch_size, shuffle=<span class="hljs-literal">True</span>)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">val_dataloader</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> DataLoader(self.cifar_val, batch_size=self.batch_size)

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test_dataloader</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> DataLoader(self.cifar_test, batch_size=self.batch_size)
</code></pre>
<h2 id="implementing-a-cnn">Implementing a CNN</h2>
<p>Several CNN architectures are implemented using the PyTorch framework. Each architecture consists of convolutional layers, pooling layers, and fully connected layers. The architectures vary in depth, number of filters, and regularization techniques.</p>
<h3 id="architecture-1-moderate-depth-with-dropout">Architecture 1: Moderate Depth with Dropout</h3>
<ul>
<li>Three convolutional layers with increasing number of filters (32, 64, 128) and kernel size of 3x3 with padding of 1</li>
<li>Max pooling layers after each convolutional layer with pool size of 2x2</li>
<li>Flatten layer to convert the feature maps into a 1D vector</li>
<li>Two fully connected layers with 512 and 10 units respectively</li>
<li>Dropout layer with a probability of 0.5 after the first fully connected layer</li>
</ul>
<h3 id="architecture-2-simplicity-and-efficiency">Architecture 2: Simplicity and Efficiency</h3>
<ul>
<li>Two convolutional layers with 32 and 64 filters respectively and kernel size of 3x3 with padding of 1</li>
<li>Max pooling layer after the second convolutional layer with pool size of 2x2</li>
<li>Flatten layer to convert the feature maps into a 1D vector</li>
<li>Two fully connected layers with 256 and 10 units respectively</li>
<li>Dropout layer with a probability of 0.5 after the first fully connected layer</li>
</ul>
<h3 id="architecture-3-enhanced-feature-normalization">Architecture 3: Enhanced Feature Normalization</h3>
<ul>
<li>Four convolutional layers with 32, 32, 64, and 64 filters respectively and kernel size of 3x3 with padding of 1</li>
<li>Batch normalization layers after each convolutional layer</li>
<li>Max pooling layers after the third and fourth convolutional layers with pool size of 2x2</li>
<li>Flatten layer to convert the feature maps into a 1D vector</li>
<li>Two fully connected layers with 256 and 10 units respectively</li>
<li>Dropout layer with a probability of 0.5 after the first fully connected layer</li>
</ul>
<h3 id="architecture-4-resnet-with-basicblocks">Architecture 4: ResNet with BasicBlocks</h3>
<ul>
<li>ResNet architecture with BasicBlock as the building block</li>
<li>Three layers: layer1, layer2, and layer3 with increasing number of filters (16, 32, 64) and stride of 1, 2, 2 respectively</li>
<li>Each layer consists of a specified number of BasicBlocks (2, 2, 2 in this case)</li>
<li>BasicBlock contains two convolutional layers with batch normalization and ReLU activation</li>
<li>Shortcut connection is added to the BasicBlock to enable residual learning</li>
<li>Global average pooling is applied after the last layer</li>
<li>Fully connected layer with 10 units for classification</li>
</ul>
<h2 id="training-cnn">Training CNN</h2>
<p>The CNN models are trained using the training set. The training process involves:</p>
<ul>
<li>Defining the loss function (e.g., cross-entropy loss)</li>
<li>Initializing the optimizer (e.g., Adam optimizer)</li>
<li>Iterating over the training data in batches</li>
<li>Forward pass: computing the model's predictions</li>
<li>Calculating the loss between the predictions and the true labels</li>
<li>Backward pass: computing gradients and updating the model's parameters</li>
<li>Monitoring the training loss and accuracy</li>
</ul>
<pre><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.avg_pool2d(out, <span class="hljs-number">8</span>)  <span class="hljs-comment"># Global Average Pooling</span>
        out = out.view(out.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)  <span class="hljs-comment"># Flatten the output</span>
        out = self.linear(out)
        <span class="hljs-keyword">return</span> out

<span class="hljs-keyword">def</span> <span class="hljs-title function_">training_step</span>(<span class="hljs-params">self, batch, batch_idx</span>):
    images, labels = batch
    outputs = self(images)
    loss = F.cross_entropy(outputs, labels)
    self.train_losses.append(loss)
    self.log(<span class="hljs-string">&#x27;train_loss&#x27;</span>, loss)
    self.log(<span class="hljs-string">&#x27;train_acc&#x27;</span>, self.accuracy(outputs, labels))
    <span class="hljs-keyword">return</span> loss
</code></pre>
<p>Loss plot for ResNet with BasicBlocks (Architecture 4):</p>
<p><img src="file:////Users/krishan/Documents/GitHub/Image-Classification-with-Convolutional-Neural-Networks/resnet_loss_plot.png" alt="Loss Plot"></p>
<h2 id="evaluating-model">Evaluating Model</h2>
<p>The trained models are evaluated on the test set to measure their performance. The evaluation metrics include:</p>
<ul>
<li>Accuracy: the percentage of correctly classified images</li>
<li>Loss: the average loss value on the test set</li>
<li>Precision: the proportion of true positive predictions among the positive predictions</li>
<li>Recall: the proportion of true positive predictions among the actual positive instances</li>
<li>F1 score: the harmonic mean of precision and recall</li>
</ul>
<pre><code class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">validation_step</span>(<span class="hljs-params">self, batch, batch_idx</span>):
        images, labels = batch
        out = self(images)
        loss = F.cross_entropy(out, labels)
        acc = accuracy(out, labels, task=<span class="hljs-string">&quot;multiclass&quot;</span>, num_classes=<span class="hljs-number">10</span>)
        self.log(<span class="hljs-string">&#x27;val_loss&#x27;</span>, loss)
        self.log(<span class="hljs-string">&#x27;val_accuracy&#x27;</span>, acc)
</code></pre>
<h2 id="results">Results</h2>
<h3 id="architecture-1-moderate-depth-with-dropout-1">Architecture 1: Moderate Depth with Dropout</h3>
<ul>
<li>Average training loss: 0.9802</li>
<li>Validation loss: 0.8916</li>
<li>Validation accuracy: 68.26%</li>
</ul>
<h3 id="architecture-2-simplicity-and-efficiency-1">Architecture 2: Simplicity and Efficiency</h3>
<ul>
<li>Average training loss: 0.8396</li>
<li>Validation loss: 0.8115</li>
<li>Validation accuracy: 70.68%</li>
</ul>
<h3 id="architecture-3-enhanced-feature-normalization-1">Architecture 3: Enhanced Feature Normalization</h3>
<ul>
<li>Average training loss: 0.6413</li>
<li>Validation loss: 0.6561</li>
<li>Validation accuracy: 77.68%</li>
</ul>
<h3 id="architecture-4-resnet-with-basicblocks-1">Architecture 4: ResNet with BasicBlocks</h3>
<ul>
<li>Validation loss: 0.5208</li>
<li>Validation accuracy: 80.80%</li>
<li>Validation F1 score: 0.7875</li>
<li>Validation precision: 0.8120</li>
<li>Validation recall: 0.8080</li>
</ul>
<h3 id="test-set-performance-on-the-best-model-architecture-4">Test Set Performance on the Best Model (Architecture 4)</h3>
<ul>
<li>Test loss: 0.5851</li>
<li>Test accuracy: 78.35%</li>
<li>Test F1 score: 0.7608</li>
<li>Test precision: 0.7854</li>
<li>Test recall: 0.7835</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>In this project, we implemented and compared different CNN architectures for image classification on the CIFAR-10 dataset. The ResNet architecture with BasicBlocks (Architecture 4) achieved the best performance with a test accuracy of 78.35% and an F1 score of 0.7608. The use of residual learning, deeper architecture, and global average pooling contributed to its superior performance compared to the other architectures.</p>
<p>The project demonstrates the effectiveness of CNNs for image classification tasks and highlights the importance of architecture design choices. Further improvements can be explored by fine-tuning hyperparameters, using data augmentation techniques, or experimenting with other advanced CNN architectures.</p>
<h2 id="references">References</h2>
<p>[1] <a href="https://youtube.com/playlist?list=PLuhqtP7jdD8CD6rOWy20INGM44kULvrHu&amp;si=Q40tqZJVGNIX_e2P">Convolution Neural Network by Coding Lane</a></p>
<p><img src="file:////Users/krishan/Documents/GitHub/Image-Classification-with-Convolutional-Neural-Networks/CNN.png" alt="RNN CNN"></p>
<p>[2] <a href="https://www.youtube.com/watch?v=Q1JCrG1bJ-A">Residual Networks and Skip Connections (DL 15) by Professor Bryce</a></p>
<p><img src="file:////Users/krishan/Documents/GitHub/Image-Classification-with-Convolutional-Neural-Networks/ResNEt.png" alt="Resnet"></p>

            
            
        </body>
        </html>